{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1kSTDFUlXRDuBju58WNGFFAEpZY-XLNpP","authorship_tag":"ABX9TyNSO0p7wDeAex65QBKvB93I"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"oaG_sZ6Hwaqr","executionInfo":{"status":"ok","timestamp":1688740612500,"user_tz":-540,"elapsed":17461,"user":{"displayName":"chaetwo mooon","userId":"03740964430699414311"}}},"outputs":[],"source":["import os\n","import cv2\n","import pandas as pd\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import torch.nn.functional as F\n","\n","from tqdm import tqdm\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","source":["PATH='/content/drive/MyDrive/공동 AI 경진대회/2023_SW_AI_Competition/open/'"],"metadata":{"id":"0ZMNaCUfwhWa","executionInfo":{"status":"ok","timestamp":1688740669626,"user_tz":-540,"elapsed":553,"user":{"displayName":"chaetwo mooon","userId":"03740964430699414311"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# RLE 디코딩 함수\n","def rle_decode(mask_rle, shape):\n","    s = mask_rle.split()\n","    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n","    starts -= 1\n","    ends = starts + lengths\n","    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n","    for lo, hi in zip(starts, ends):\n","        img[lo:hi] = 1\n","    return img.reshape(shape)\n","\n","# RLE 인코딩 함수\n","def rle_encode(mask):\n","    pixels = mask.flatten()\n","    pixels = np.concatenate([[0], pixels, [0]])\n","    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n","    runs[1::2] -= runs[::2]\n","    return ' '.join(str(x) for x in runs)"],"metadata":{"id":"NXt2B1_Mwhro","executionInfo":{"status":"ok","timestamp":1688740673476,"user_tz":-540,"elapsed":576,"user":{"displayName":"chaetwo mooon","userId":"03740964430699414311"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class SatelliteDataset(Dataset):\n","    def __init__(self, csv_file, transform=None, infer=False):\n","        self.data = pd.read_csv(csv_file)\n","        self.transform = transform\n","        self.infer = infer\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        img_path = PATH + self.data.iloc[idx, 1]\n","        image = cv2.imread(img_path)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","        if self.infer:\n","            if self.transform:\n","                image = self.transform(image=image)['image']\n","            return image\n","\n","        mask_rle = self.data.iloc[idx, 2]\n","        mask = rle_decode(mask_rle, (image.shape[0], image.shape[1]))\n","\n","        if self.transform:\n","            augmented = self.transform(image=image, mask=mask)\n","            image = augmented['image']\n","            mask = augmented['mask']\n","\n","        return image, mask"],"metadata":{"id":"bSOw8LnXwjG-","executionInfo":{"status":"ok","timestamp":1688740675142,"user_tz":-540,"elapsed":2,"user":{"displayName":"chaetwo mooon","userId":"03740964430699414311"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["transform = A.Compose(\n","    [\n","        A.Resize(256, 256),\n","        A.Normalize(),\n","        ToTensorV2()\n","    ]\n",")\n","dataset = SatelliteDataset(csv_file=PATH + './train.csv', transform=transform)\n","dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=4)"],"metadata":{"id":"lROA2eOVwkwx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688740684153,"user_tz":-540,"elapsed":4777,"user":{"displayName":"chaetwo mooon","userId":"03740964430699414311"}},"outputId":"397a717e-2b7e-4631-a948-76a153bb01ae"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}]},{"cell_type":"code","source":["# class ConvBlock(nn.Module):\n","#     def __init__(self, in_channels, out_channels):\n","#         super(ConvBlock, self).__init__()\n","#         self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n","#         self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n","\n","#     def forward(self, x):\n","#         out = F.relu(self.conv1(x))\n","#         out = F.relu(self.conv2(out))\n","#         return out\n","\n","# class UNetPlusPlus(nn.Module):\n","#     def __init__(self, in_channels, out_channels):\n","#         super(UNetPlusPlus, self).__init__()\n","#         self.conv1 = ConvBlock(in_channels, 64)\n","#         self.conv2 = ConvBlock(64, 128)\n","#         self.conv3 = ConvBlock(128, 256)\n","#         self.conv4 = ConvBlock(256, 512)\n","#         self.conv5 = ConvBlock(512, 1024)\n","\n","#         self.upconv6 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n","#         self.conv6 = ConvBlock(1024, 512)\n","\n","#         self.upconv7 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n","#         self.conv7 = ConvBlock(512, 256)\n","\n","#         self.upconv8 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n","#         self.conv8 = ConvBlock(256, 128)\n","\n","#         self.upconv9 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n","#         self.conv9 = ConvBlock(128, 64)\n","\n","#         self.output = nn.Conv2d(64, out_channels, kernel_size=1)\n","\n","#     def forward(self, x):\n","#         conv1 = self.conv1(x)\n","#         pool1 = F.max_pool2d(conv1, kernel_size=2, stride=2)\n","\n","#         conv2 = self.conv2(pool1)\n","#         pool2 = F.max_pool2d(conv2, kernel_size=2, stride=2)\n","\n","#         conv3 = self.conv3(pool2)\n","#         pool3 = F.max_pool2d(conv3, kernel_size=2, stride=2)\n","\n","#         conv4 = self.conv4(pool3)\n","#         pool4 = F.max_pool2d(conv4, kernel_size=2, stride=2)\n","\n","#         conv5 = self.conv5(pool4)\n","\n","#         upconv6 = self.upconv6(conv5)\n","#         concat6 = torch.cat([upconv6, conv4], dim=1)\n","#         conv6 = self.conv6(concat6)\n","\n","#         upconv7 = self.upconv7(conv6)\n","#         concat7 = torch.cat([upconv7, conv3], dim=1)\n","#         conv7 = self.conv7(concat7)\n","\n","#         upconv8 = self.upconv8(conv7)\n","#         concat8 = torch.cat([upconv8, conv2], dim=1)\n","#         conv8 = self.conv8(concat8)\n","\n","#         upconv9 = self.upconv9(conv8)\n","#         concat9 = torch.cat([upconv9, conv1], dim=1)\n","#         conv9 = self.conv9(concat9)\n","\n","#         output = self.output(conv9)\n","#         output = torch.sigmoid(output)\n","\n","#         return output"],"metadata":{"id":"szZ8WQ02zx2x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class ConvBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n","        super(ConvBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, padding=padding)\n","        self.bn = nn.BatchNorm2d(out_channels)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn(self.conv1(x)))\n","        out = F.relu(self.bn(self.conv2(out)))\n","        return out\n","\n","class UNetPlusPlus(nn.Module):\n","    def __init__(self, in_channels, out_channels, deep_supervision=False):\n","        super(UNetPlusPlus, self).__init__()\n","        self.deep_supervision = deep_supervision\n","\n","        self.conv1 = ConvBlock(in_channels, 64)\n","        self.conv2 = ConvBlock(64, 128)\n","        self.conv3 = ConvBlock(128, 256)\n","        self.conv4 = ConvBlock(256, 512)\n","        self.conv5 = ConvBlock(512, 1024)\n","\n","        self.upconv6 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n","        self.conv6 = ConvBlock(1024, 512)\n","\n","        self.upconv7 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n","        self.conv7 = ConvBlock(512, 256)\n","\n","        self.upconv8 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n","        self.conv8 = ConvBlock(256, 128)\n","\n","        self.upconv9 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n","        self.conv9 = ConvBlock(128, 64)\n","\n","        if self.deep_supervision:\n","            self.outconv1 = nn.Conv2d(64, out_channels, kernel_size=1)\n","            self.outconv2 = nn.Conv2d(128, out_channels, kernel_size=1)\n","            self.outconv3 = nn.Conv2d(256, out_channels, kernel_size=1)\n","            self.outconv4 = nn.Conv2d(512, out_channels, kernel_size=1)\n","\n","        self.outconv = nn.Conv2d(64, out_channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        conv1 = self.conv1(x)\n","        pool1 = F.max_pool2d(conv1, kernel_size=2, stride=2)\n","\n","        conv2 = self.conv2(pool1)\n","        pool2 = F.max_pool2d(conv2, kernel_size=2, stride=2)\n","\n","        conv3 = self.conv3(pool2)\n","        pool3 = F.max_pool2d(conv3, kernel_size=2, stride=2)\n","\n","        conv4 = self.conv4(pool3)\n","        pool4 = F.max_pool2d(conv4, kernel_size=2, stride=2)\n","\n","        conv5 = self.conv5(pool4)\n","\n","        upconv6 = self.upconv6(conv5)\n","        concat6 = torch.cat([upconv6, conv4], dim=1)\n","        conv6 = self.conv6(concat6)\n","\n","        upconv7 = self.upconv7(conv6)\n","        concat7 = torch.cat([upconv7, conv3], dim=1)\n","        conv7 = self.conv7(concat7)\n","\n","        upconv8 = self.upconv8(conv7)\n","        concat8 = torch.cat([upconv8, conv2], dim=1)\n","        conv8 = self.conv8(concat8)\n","\n","        upconv9 = self.upconv9(conv8)\n","        concat9 = torch.cat([upconv9, conv1], dim=1)\n","        conv9 = self.conv9(concat9)\n","\n","        if self.deep_supervision:\n","            out1 = self.outconv1(conv9)\n","            out2 = self.outconv2(conv8)\n","            out3 = self.outconv3(conv7)\n","            out4 = self.outconv4(conv6)\n","\n","            return [self.outconv(conv9), out4, out3, out2, out1]\n","        else:\n","            output = self.outconv(conv9)\n","            output = torch.sigmoid(output)\n","\n","            return output"],"metadata":{"id":"N1DHaU4xwnfY","executionInfo":{"status":"ok","timestamp":1688740976212,"user_tz":-540,"elapsed":556,"user":{"displayName":"chaetwo mooon","userId":"03740964430699414311"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# model 초기화\n","input_channels = 3\n","output_channels = 1\n","model = UNetPlusPlus(input_channels,output_channels).to(device)\n","\n","# loss function과 optimizer 정의\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# training loop\n","for epoch in range(100):  # 10 에폭 동안 학습합니다.\n","    model.train()\n","    epoch_loss = 0\n","    for images, masks in tqdm(dataloader):\n","        images = images.float().to(device)\n","        masks = masks.float().to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, masks.unsqueeze(1))\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    print(f'Epoch {epoch+1}, Loss: {epoch_loss/len(dataloader)}')"],"metadata":{"id":"W3lmwYT6w0u4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_dataset = SatelliteDataset(csv_file=PATH + './test.csv', transform=transform, infer=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)"],"metadata":{"id":"x4zbFi9Dw0-p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","    model.eval()\n","    result = []\n","    for images in tqdm(test_dataloader):\n","        images = images.float().to(device)\n","\n","        outputs = model(images)\n","        masks = torch.sigmoid(outputs).cpu().numpy()\n","        masks = np.squeeze(masks, axis=1)\n","        masks = (masks > 0.35).astype(np.uint8) # Threshold = 0.35\n","\n","        for i in range(len(images)):\n","            mask_rle = rle_encode(masks[i])\n","            if mask_rle == '': # 예측된 건물 픽셀이 아예 없는 경우 -1\n","                result.append(-1)\n","            else:\n","                result.append(mask_rle)"],"metadata":{"id":"1P-t8SIkw2PC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submit = pd.read_csv('./sample_submission.csv')\n","submit['mask_rle'] = result"],"metadata":{"id":"fpur8mrEw32p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submit.to_csv('./submit.csv', index=False)"],"metadata":{"id":"w0wGzIUPw41i"},"execution_count":null,"outputs":[]}]}